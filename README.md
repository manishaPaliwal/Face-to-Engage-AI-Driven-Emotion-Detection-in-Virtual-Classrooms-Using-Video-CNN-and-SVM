# ğŸ¥ **Face-to-Engage: AI-Driven Emotion Detection in Virtual Classrooms Using Video, CNN, and SVM**

## ğŸ“Œ **Project Overview:**

**Face-to-Engage** builds an AI-powered solution to detect **engagement, boredom, frustration, and confusion** from facial expressions in virtual settings like online classrooms and meetings. Using the **DAiSEE dataset**, we applied and compared **handcrafted feature extraction (Dlib 68 facial landmarks)** and **CNN-based deep features**, feeding both into **SVM classifiers** to evaluate emotion recognition performance.

This project has practical applications for **e-learning platforms, virtual training, and online collaboration tools** aiming to measure real-time user attention and emotional states.

---

## ğŸ—ï¸ **Tech Stack & Their Roles:**

âœ… **AWS S3** â€“ Stored 15GB DAiSEE video dataset and processed image frames  
âœ… **AWS SageMaker** â€“ Modeled, trained, and evaluated ML pipelines via Jupyter notebooks  
âœ… **Python** â€“ Core programming language for data wrangling, modeling, evaluation  
âœ… **TensorFlow (Keras API)** â€“ Built custom CNN architecture for deep feature extraction  
âœ… **OpenCV** â€“ Processed video frames: reading, writing, image manipulation  
âœ… **Dlib** â€“ Extracted **68 facial landmarks** as handcrafted features  
âœ… **Scikit-learn (SVM-SVC)** â€“ Classifier for predicting emotional states  
âœ… **Matplotlib** â€“ Visualized training loss, validation loss, confusion matrices  

---

## ğŸ’¡ **What Makes This Project Unique:**

âœ¨ Explored **dual feature extraction pipelines** â†’ handcrafted features via Dlib vs CNN features via Keras  
âœ¨ Tackled **multi-label, multi-intensity classification problem** (emotion + intensity levels 0â€“3)  
âœ¨ Deployed and ran models fully on **AWS cloud (S3 + SageMaker)**  
âœ¨ Compared per-emotion accuracy (engagement, boredom, confusion, frustration) with actionable visualizations  
âœ¨ Evaluated **interpretable models (SVM) instead of black-box deep neural networks** for better explainability

---

## ğŸ“Š **Key Results & Insights:**

âœ”ï¸ **CNN+SVM outperformed handcrafted features â†’ 50% accuracy vs 45%**  
âœ”ï¸ Frustration detection achieved **highest individual class accuracy (80%)**  
âœ”ï¸ Confusion was **most difficult emotion to classify** across both models  
âœ”ï¸ Validation loss higher than training â†’ dataset underfitting, room for improvement  
âœ”ï¸ Combining CNN + handcrafted features (hybrid models in literature) shows benchmark accuracy of 53%

---

## âœ… **What Went Well:**

ğŸŒŸ Successfully implemented **two complete ML pipelines: Dlib landmarks + SVM, CNN features + SVM**  
ğŸŒŸ Leveraged **AWS SageMaker and S3 for scalable cloud ML workflow**  
ğŸŒŸ Achieved consistent performance despite small, noisy dataset  
ğŸŒŸ Generated per-emotion **confusion matrices and insights into which emotions are harder to predict**

---

## âš ï¸ **Challenges:**

ğŸ˜… Processing **large video dataset (15GB)** â†’ required frame extraction and batching  
ğŸ˜… **Parsing nested intensity labels** for multi-class SVM setup  
ğŸ˜… SVM doesnâ€™t natively handle multi-label â†’ implemented **one-vs-one classifiers per emotion**  
ğŸ˜… Limited annotated data â†’ test set excluded due to missing labels  
ğŸ˜… Balancing training/validation splits to handle **class imbalance**

---

## ğŸ§© **Methodology Overview:**

1. Converted video â†’ image frames
2. **Facial Landmark Pipeline:**
   - Extracted **68 landmarks via Dlib**
   - Computed Euclidean distances
   - Fed vectors into **SVM-SVC classifier**
3. **CNN Pipeline:**
   - Built CNN (Conv2D layers, LeakyReLU, Softmax)
   - Extracted deep features
   - Fed features into **SVM-SVC classifier**
4. Evaluated both models on accuracy, precision, recall, confusion matrices

---

## ğŸ“ **Evaluation Metrics:**

| Model                     | Accuracy | Precision | Recall |
|--------------------------|----------|-----------|--------|
| 68 Facial Landmarks + SVM | 0.45     | 0.45      | 0.45   |
| CNN Features + SVM        | 0.50     | 0.50      | 0.50   |

âœ… Frustration class achieved best accuracy; confusion class remained hardest to detect.

